{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import json\n",
    "import os\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ACER\\\\Documents\\\\My mini Projects\\\\ML\\\\Drawing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_from_csv(filename: str) -> str:\n",
    "    \"\"\"\n",
    "        This method will extract the label from the file name\n",
    "        Ex:\n",
    "            from `elephant.csv` -> 'elephant'\n",
    "    \"\"\"\n",
    "    return filename.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simplified():\n",
    "    \"\"\"\n",
    "        Preprocess the csv files\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_path='dataset'):\n",
    "        self.input_path = input_path\n",
    "\n",
    "    def list_all_categories(self):\n",
    "        \"\"\" \n",
    "            Returns list of all categories available in the csvs, by considering csv file name\n",
    "        \"\"\"        \n",
    "        files = os.listdir(os.path.join(cwd, self.input_path))\n",
    "        return sorted([extract_label_from_csv(f) for f in files], key=str.lower)\n",
    "\n",
    "    def read_training_csv(self, category, nrows=None, usecols=None, drawing_transform=False):\n",
    "        \"\"\"\n",
    "            category: str, compulsury\n",
    "                Category in other words csv file name needs to read\n",
    "            nrows:int, optional\n",
    "                Number of rows of file to read. Useful for reading pieces of large files.\n",
    "            usecols: list-like or callable, optional\n",
    "                Return a subset of the columns. If list-like, all elements must either be \n",
    "                positional (i.e. integer indices into the document columns) or strings that\n",
    "                correspond to column names provided either by the user in names or inferred \n",
    "                from the document header row(s)\n",
    "            drawing_transform: bool, optional\n",
    "                Whether the returning dataframe got structured drawing cordinates or str\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(self.input_path, category + '.csv'),\n",
    "                         nrows=nrows, parse_dates=['timestamp'], usecols=usecols)\n",
    "        if drawing_transform:\n",
    "            df['drawing'] = df['drawing'].apply(json.loads)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "s = Simplified('dataset')\n",
    "NCSVS = 100 # number of shuffled files\n",
    "categories = s.list_all_categories()\n",
    "print(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:04,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for y, cat in tqdm(enumerate(categories)):\n",
    "    df = s.read_training_csv(cat, nrows=30000)\n",
    "    # add column y with category id comming from emumerate\n",
    "    df['y'] = y\n",
    "    # add cv column by calculating random number using key_id\n",
    "    df['cv'] = (df['key_id'] // 10 ** 7) % NCSVS\n",
    "    \n",
    "    for k in range(NCSVS):\n",
    "        # create csv name using 1-NCSVS\n",
    "        filename = 'train_k{}.csv'.format(k)\n",
    "        # keep records where cv == k\n",
    "        chunk = df[df['cv'] == k]\n",
    "        chunk = chunk.drop(['key_id'], axis=1)\n",
    "        \n",
    "        if y == 0:\n",
    "            chunk.to_csv('shuffled_csv/' + filename, index=False)\n",
    "        else:\n",
    "            chunk.to_csv('shuffled_csv/' + filename, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1519, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(NCSVS)):\n",
    "    filename = 'shuffled_csv/train_k{}.csv'.format(k)\n",
    "    \n",
    "    # if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        # make a column with randomly assigned numbers\n",
    "        df['rnd'] = np.random.rand(len(df))\n",
    "        # sort by rnd column\n",
    "        df = df.sort_values(by='rnd').drop('rnd', axis=1)\n",
    "        df.to_csv(filename + '.gz', compression='gzip', index=False)\n",
    "        os.remove(filename)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = 163\n",
    "size = 64\n",
    "STEPS = 1000\n",
    "batchsize = 512\n",
    "epochs = 15\n",
    "\n",
    "def draw_cv2(raw_strokes, size=256, lw=6):\n",
    "    img = np.zeros((64, 64), np.uint8)\n",
    "    for stroke in raw_strokes:\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), 255, lw)\n",
    "    if size != 256:\n",
    "        return cv2.resize(img, (size, size))\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "#ADD DATA AUGMENTATION TO BOOST\n",
    "def image_generator(size, batchsize, ks, lw=6):\n",
    "    cnt = 0\n",
    "    \n",
    "    while True:\n",
    "        for k in np.random.permutation(ks):\n",
    "            \n",
    "            filename = os.path.join(cwd, 'shuffled_csv/train_k{}.csv.gz'.format(k))\n",
    "            \n",
    "            for df in pd.read_csv(filename, chunksize=batchsize):\n",
    "                df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "                \n",
    "                x = np.zeros((len(df), size, size))\n",
    "                \n",
    "                for i, raw_strokes in enumerate(df.drawing.values):\n",
    "                    x[i] = draw_cv2(raw_strokes, size=size, lw=lw)\n",
    "                \n",
    "                # normalize the image arr    \n",
    "                x = x / 255.\n",
    "                # reshapes the arr\n",
    "                x = x.reshape((len(df), size, size, 1)).astype(np.float32)\n",
    "                # encode categories\n",
    "                y = tf.keras.utils.to_categorical(df.y, num_classes=num_of_classes)\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt == batchsize :\n",
    "                    cnt=0  #don't forget to set this number to 0\n",
    "                    yield x, y\n",
    "                \n",
    "\n",
    "def df_to_image_array(df, size=size, lw=6):\n",
    "#     df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "    x = np.zeros((len(df), size, size))\n",
    "    for i, raw_strokes in enumerate(df.drawing.values):\n",
    "        x[i] = draw_cv2(raw_strokes)\n",
    "    x = x / 255.\n",
    "    x = x.reshape((len(df), size, size, 1)).astype(np.float32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1519, 64, 64, 1) (1519, 163)\n",
      "Validation array memory 0.02 GB\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "valid_df = pd.read_csv(os.path.join(cwd, 'shuffled_csv/train_k{}.csv.gz'.format(NCSVS - 1)), nrows=30000)\n",
    "x_valid = df_to_image_array(valid_df, size)\n",
    "y_valid = tf.keras.utils.to_categorical(valid_df.y, num_classes=num_of_classes)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print('Validation array memory {:.2f} GB'.format(x_valid.nbytes / 1024.**3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = image_generator(size=size, batchsize=batchsize, ks=range(NCSVS - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D  (None, 32, 32, 32)        0         \n",
      ")                                                                \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling  (None, 16, 16, 64)        0         \n",
      "2D)                                                              \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 680)               11141800  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 680)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 163)               111003    \n",
      "=================================================================\n",
      "Total params: 11,271,619\n",
      "Trainable params: 11,271,619\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(64, 64, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(680, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_of_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\anaconda3\\envs\\venv_gpu\\lib\\site-packages\\keras\\engine\\training.py:1967: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   7/1000 [..............................] - ETA: 23:19:49 - loss: 2.9047 - accuracy: 0.2689"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    train_datagen, steps_per_epoch=STEPS, epochs=epochs, verbose=1,\n",
    "    validation_data=(x_valid, y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
